{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49552914",
   "metadata": {},
   "source": [
    "# Trabajo Integrador: DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d9cd1",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install duckdb --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335237e5",
   "metadata": {},
   "source": [
    "### Creando conexión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619f2e2",
   "metadata": {},
   "source": [
    "Nos permite establecer una conexión a una base de datos, por defecto, si no especificamos su nombre, la base de datos no persistirá y operará en memoria, por lo tanto no se almacenarán las tablas creadas. Trabajaremos en memoria ya que consideramos que es el fuerte de DUCKDB y su análisis es el pertinente de este trabajo de investigación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75c5e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as db\n",
    "\n",
    "database = db.connect(database=\":memory:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48b092",
   "metadata": {},
   "source": [
    "##### Prueba con el dataset. En PostgreSQL almacenaremos archivos de órdenes y sus pagos. En AWS almacenaremos los productos y su categoría. El resto de archivos será almacenado de manera local, algunos en CSV y otros en PARQUET. Esta información se encuentra representada mediante un esquema en la documentación adjunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6441cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q kagglehub        \n",
    "import kagglehub, shutil, pathlib\n",
    "\n",
    "path = kagglehub.dataset_download(\"olistbr/brazilian-ecommerce\")\n",
    "\n",
    "local_csv_files = [\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    ## Estos archivos se almacenaran en PSQL \n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\", \n",
    "]\n",
    "\n",
    "for file_name in local_csv_files:\n",
    "    shutil.copy(f\"{path}/{file_name}\", f\"dataset/{file_name}\")\n",
    "\n",
    "for file_name in local_csv_files:\n",
    "    table = pathlib.Path(f\"dataset/{file_name}\").stem\n",
    "    database.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {table} AS\n",
    "        SELECT * FROM read_csv_auto('dataset/{file_name}');\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47020f3e",
   "metadata": {},
   "source": [
    "#### Verificación básica de la creación de las tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in local_csv_files:\n",
    "    table = pathlib.Path(f\"dataset/{file_name}\").stem\n",
    "    schema = database.sql(f\"DESCRIBE {table}\")\n",
    "    print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23715818",
   "metadata": {},
   "source": [
    "#### Obteniendo los datos de forma remota - Conexión con AWS S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936ae9d",
   "metadata": {},
   "source": [
    "**NOTA:** Para utilizar AWS S3 es necesario setear las credenciales para acceder al bucket, para eso debemos crear un user en la IAM de AWS, asignarle permisos y finalmente crear las claves de acceso para este usuario. Luego, estas credenciales son obtenidas desde un .env.\n",
    "Es importante que la región del bucket y del usuario sean la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b45604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "import os\n",
    "\n",
    "database.sql(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "\n",
    "database.sql(f\"\"\"\n",
    "SET s3_region='{os.getenv(\"AWS_REGION\")}';\n",
    "SET s3_access_key_id='{os.getenv(\"AWS_ACCESS_KEY_ID\")}';\n",
    "SET s3_secret_access_key='{os.getenv(\"AWS_SECRET_ACCESS_KEY\")}';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7dfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c54f8be",
   "metadata": {},
   "source": [
    "## Operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ee7dd",
   "metadata": {},
   "source": [
    "### Creando las tablas con los datos de forma remota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "adde13fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.sql(f\"\"\"\n",
    " CREATE OR REPLACE TABLE olist_products_dataset AS\n",
    "        SELECT * FROM read_csv_auto('s3://ti-quadrelli-ribarov/olist_products_dataset.csv');\n",
    "\"\"\")\n",
    "\n",
    "database.sql(f\"\"\"\n",
    " CREATE OR REPLACE TABLE product_category_name_translation AS\n",
    "        SELECT * FROM read_csv_auto('s3://ti-quadrelli-ribarov/product_category_name_translation.csv');\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b1017",
   "metadata": {},
   "source": [
    "#### Validación básica de la creación de las tablas con los datos remotos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_csv_files  = [\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\",\n",
    "]\n",
    "\n",
    "for file_name in remote_csv_files:\n",
    "    table = pathlib.Path(f\"dataset/{file_name}\").stem\n",
    "    schema = database.sql(f\"DESCRIBE {table}\")\n",
    "    print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536cfe15",
   "metadata": {},
   "source": [
    "#### Obteniendo los datos desde PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0d62e",
   "metadata": {},
   "source": [
    "#### Nota: este paso se podría haber hecho sin DuckDB, mismo desde un manejador de base de datos como DBeaver o con la librería Pandas, sin embargo, nuevamente debido al objetivo de este proyecto, se eligió realizarlo utilizando DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5003d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Esta celda utiliza duckdb para conectarse a PostgreSQL, crea la base de datos trabajo integrador\n",
    "# en caso de que no exista \n",
    "\n",
    "database.execute(\"INSTALL postgres;\")\n",
    "database.execute(\"LOAD postgres;\")\n",
    "\n",
    "try:\n",
    "    database.execute(\"DETACH pgadmin;\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "conninfo = f\"host={os.getenv('PG_HOST')} port={os.getenv('PG_PORT')} user={os.getenv('PG_USER')} password={os.getenv('PG_PASSWORD')} dbname=postgres\"\n",
    "database.execute(f\"ATTACH '{conninfo}' AS pgadmin (TYPE postgres);\")\n",
    "\n",
    "exists = database.execute(\"\"\"\n",
    "    SELECT COUNT(*) > 0\n",
    "    FROM postgres_query(\n",
    "        'pgadmin',\n",
    "        $$SELECT 1 FROM pg_database WHERE datname = 'trabajo_integrador'$$\n",
    "    );\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "if not exists:\n",
    "    database.execute(\"\"\"\n",
    "        CALL postgres_execute(\n",
    "            'pgadmin',\n",
    "            $$CREATE DATABASE trabajo_integrador$$,\n",
    "            use_transaction => false\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "database.execute(\"DETACH pgadmin;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79839974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os\n",
    "\n",
    "orders_csv   = pathlib.Path(\"olist_orders_dataset.csv\")\n",
    "payments_dataset = pathlib.Path(\"olist_order_payments_dataset.csv\")\n",
    "   \n",
    "\n",
    "conninfo = f\"host={os.getenv(\"PG_HOST\")} port={os.getenv(\"PG_PORT\")} user={os.getenv(\"PG_USER\")} password={os.getenv(\"PG_PASSWORD\")} dbname={os.getenv(\"PG_DB\")}\"\n",
    "database.execute(f\"ATTACH '{conninfo}' AS pgdb (TYPE postgres);\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630345f",
   "metadata": {},
   "source": [
    "#### Creación de tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acf563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "orders_csv = pathlib.Path(path) / \"olist_orders_dataset.csv\"\n",
    "payments_dataset = pathlib.Path(path) / \"olist_order_payments_dataset.csv\"\n",
    "\n",
    "database.execute(f\"\"\"\n",
    "    DROP TABLE IF EXISTS pgdb.olist_orders;\n",
    "    CREATE TABLE pgdb.olist_orders AS\n",
    "    SELECT *\n",
    "    FROM read_csv_auto('{orders_csv.as_posix()}', HEADER=TRUE);\n",
    "\"\"\")\n",
    "\n",
    "database.execute(f\"\"\"\n",
    "    DROP TABLE IF EXISTS pgdb.olist_orders_payments;\n",
    "    CREATE TABLE pgdb.olist_orders_payments AS\n",
    "    SELECT *\n",
    "    FROM read_csv_auto('{payments_dataset.as_posix()}', HEADER=TRUE);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgreee_csv_files  = [\n",
    "   \"olist_orders\", \n",
    "   \"olist_orders_payments\"\n",
    "]\n",
    "\n",
    "for file_name in postgreee_csv_files:\n",
    "    table = pathlib.Path(f\"dataset/{file_name}\").stem\n",
    "    schema = database.sql(f\"DESCRIBE pgdb.{table}\")\n",
    "    print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac1742f",
   "metadata": {},
   "source": [
    "## Exploracion del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311940c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.sql(\"SHOW TABLES\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bae5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.sql(\"SELECT * FROM olist_customers_dataset LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3711f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filas con nulos:\")\n",
    "database.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM olist_customers_dataset\n",
    "WHERE\n",
    "    customer_id IS NULL OR\n",
    "    customer_unique_id IS NULL OR\n",
    "    customer_zip_code_prefix IS NULL OR\n",
    "    customer_city IS NULL OR\n",
    "    customer_state IS NULL\n",
    "LIMIT 5\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb357ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.sql(\"SELECT * FROM olist_geolocation_dataset LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e753f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = database.sql(\"SHOW TABLES\").df()['name'].tolist()\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"Para la tabla: {table}\")\n",
    "    columns = database.sql(f\"DESCRIBE {table}\").df()['column_name'].tolist()\n",
    "    where_clause = \" OR \".join([f\"{col} IS NULL\" for col in columns])\n",
    "    query = f\"SELECT 1 FROM {table} WHERE {where_clause} LIMIT 1\"\n",
    "    result = database.sql(query).df()\n",
    "    if not result.empty:\n",
    "        print(\"Tiene nulos\")\n",
    "    else:\n",
    "        print(\"NO tiene nulos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86c565",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "tablas_con_nulos = [\"olist_order_reviews_dataset\", \"olist_orders_dataset\", \"olist_products_dataset\", \"olist_sellers_dataset\", \"olist_customers_dataset\"]\n",
    "for table in tablas_con_nulos:\n",
    "    print(f\"Tabla: {table}\")\n",
    "    columns = database.sql(f\"DESCRIBE {table}\").df()['column_name'].tolist()\n",
    "    where_clause = \" OR \".join([f\"{col} IS NULL\" for col in columns])\n",
    "    query = f\"SELECT * FROM {table} WHERE {where_clause} LIMIT 5\"\n",
    "    df_nulos = database.sql(query).df()\n",
    "    print(df_nulos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65360af9",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "total = database.sql(\"SELECT COUNT(*) AS total FROM olist_order_reviews_dataset\").df().iloc[0]['total']\n",
    "nulos = database.sql(\"SELECT COUNT(*) AS nulos FROM olist_order_reviews_dataset WHERE review_comment_title IS NULL\").df().iloc[0]['nulos']\n",
    "no_nulos = total - nulos\n",
    "print(f\"review_comment_title nulos: {nulos}\")\n",
    "print(f\"review_comment_title no nulos: {no_nulos}\")\n",
    "print(f\"Porcentaje de nulos: {nulos / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41abaf1",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "database.sql(\"DESCRIBE olist_orders_dataset\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec97742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la orden más vieja y la última según la columna order_purchase_timestamp\n",
    "result = database.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(order_purchase_timestamp) AS orden_mas_vieja,\n",
    "        MAX(order_purchase_timestamp) AS orden_mas_reciente\n",
    "    FROM olist_orders_dataset\n",
    "\"\"\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b31d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    EXTRACT(year FROM order_purchase_timestamp) AS anio,\n",
    "    COUNT(*) AS cantidad_ordenes\n",
    "FROM olist_orders_dataset\n",
    "GROUP BY anio\n",
    "ORDER BY cantidad_ordenes DESC\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "database.sql(query).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la categoría de producto más solicitada\n",
    "query = \"\"\"\n",
    "SELECT product_category_name, COUNT(*) AS cantidad\n",
    "FROM olist_products_dataset\n",
    "GROUP BY product_category_name\n",
    "ORDER BY cantidad DESC\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "database.sql(query).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.sql(\"\"\"\n",
    "SELECT customer_city, COUNT(*) AS cantidad\n",
    "FROM olist_customers_dataset\n",
    "GROUP BY customer_city\n",
    "ORDER BY cantidad DESC\n",
    "LIMIT 1\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39aa765",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.sql(\n",
    "\"\"\"\n",
    "SELECT seller_city, COUNT(*) AS cantidad_vendedores\n",
    "FROM olist_sellers_dataset\n",
    "GROUP BY seller_city\n",
    "ORDER BY cantidad_vendedores DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d38d7a",
   "metadata": {},
   "source": [
    "## Consultas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2ca55",
   "metadata": {},
   "source": [
    "Esta sección consiste en realizar consultas interesantes que podrían ser de utilidad para un negocio que cuenta con el dataset en estudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a2591",
   "metadata": {},
   "source": [
    "¿Cuántos días demora la entrega de las órdenes en los distintos años?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52322d6",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    EXTRACT(year FROM order_purchase_timestamp) AS anio,\n",
    "    AVG(DATEDIFF('day', order_purchase_timestamp, order_delivered_customer_date)) AS promedio_dias\n",
    "FROM olist_orders_dataset\n",
    "WHERE \n",
    "    order_delivered_customer_date IS NOT NULL\n",
    "GROUP BY anio\n",
    "ORDER BY anio DESC\n",
    "\"\"\"\n",
    "database.sql(query).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f7990",
   "metadata": {},
   "source": [
    "¿Cuántos clientes ordenaron en el sitio web?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb31dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    EXTRACT(year FROM olist_orders_dataset.order_purchase_timestamp) AS anio,\n",
    "    COUNT(DISTINCT olist_customers_dataset.customer_unique_id) AS cantidad_clientes\n",
    "FROM olist_orders_dataset\n",
    "JOIN olist_customers_dataset\n",
    "    ON olist_orders_dataset.customer_id = olist_customers_dataset.customer_id\n",
    "GROUP BY anio\n",
    "ORDER BY anio\n",
    "\"\"\"\n",
    "database.sql(query).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9d366",
   "metadata": {},
   "source": [
    "¿Cuál fue el cliente con más órdenes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390123cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT c.customer_unique_id, COUNT(*) AS n_ordenes\n",
    "FROM olist_orders_dataset o\n",
    "JOIN olist_customers_dataset c\n",
    "  ON o.customer_id = c.customer_id\n",
    "GROUP BY c.customer_unique_id\n",
    "ORDER BY n_ordenes DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "database.sql(query).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbaceb6",
   "metadata": {},
   "source": [
    "Como parte de un analisis, vamos a crear una tabla nueva con los clientes que realizaron más de 3 pedidos en un año y luego la consultaremos para saber si estos clientes siguieron realizando pedidos o abandonaron la plataforma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "04fa7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "CREATE TABLE clientes_pedidos_por_año AS\n",
    "SELECT\n",
    "    c.customer_unique_id,\n",
    "    EXTRACT(YEAR FROM o.order_purchase_timestamp) AS año,\n",
    "    COUNT(*) AS num_pedidos\n",
    "FROM olist_orders_dataset o\n",
    "JOIN olist_customers_dataset c\n",
    "  ON o.customer_id = c.customer_id\n",
    "GROUP BY c.customer_unique_id, año\n",
    "ORDER BY c.customer_unique_id, año;\n",
    "\"\"\"\n",
    "database.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbe2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM clientes_pedidos_por_año\n",
    "\"\"\"\n",
    "database.sql(query).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f3381d",
   "metadata": {},
   "source": [
    "Una buena forma de saber por qué estos clientes dejaron la plataforma es observando sus reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d3757f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE OR REPLACE TABLE clientes_perdidos AS\n",
    "WITH max_año AS (\n",
    "    SELECT MAX(año) AS ultimo_año\n",
    "    FROM clientes_pedidos_por_año\n",
    "),\n",
    "activos AS (\n",
    "    SELECT\n",
    "        cpa.customer_unique_id,\n",
    "        cpa.año AS año_activo\n",
    "    FROM clientes_pedidos_por_año cpa\n",
    "    WHERE cpa.num_pedidos > 3\n",
    "),\n",
    "posteriores AS (\n",
    "    SELECT\n",
    "        a.customer_unique_id,\n",
    "        MIN(cpa.año) AS primer_año_posterior\n",
    "    FROM activos a\n",
    "    JOIN clientes_pedidos_por_año cpa\n",
    "      ON a.customer_unique_id = cpa.customer_unique_id\n",
    "     AND cpa.año > a.año_activo\n",
    "     AND cpa.num_pedidos > 0\n",
    "    GROUP BY a.customer_unique_id, a.año_activo\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    a.customer_unique_id,\n",
    "    a.año_activo\n",
    "FROM activos a\n",
    "JOIN max_año m\n",
    "  ON 1=1\n",
    "LEFT JOIN posteriores p\n",
    "  ON a.customer_unique_id = p.customer_unique_id\n",
    " AND a.año_activo = p.primer_año_posterior - 1\n",
    "WHERE\n",
    "    a.año_activo < m.ultimo_año\n",
    "  AND p.primer_año_posterior IS NULL;\n",
    "\"\"\"\n",
    "database.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM clientes_perdidos\n",
    "\"\"\"\n",
    "database.sql(query).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a4e30",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    cp.customer_unique_id,\n",
    "    cp.año_activo AS año,\n",
    "    r.review_score AS puntaje,\n",
    "    r.review_comment_message AS comentario\n",
    "FROM clientes_perdidos cp\n",
    "JOIN olist_customers_dataset c\n",
    "    ON cp.customer_unique_id = c.customer_unique_id\n",
    "JOIN olist_orders_dataset o\n",
    "    ON c.customer_id = o.customer_id\n",
    "LEFT JOIN olist_order_reviews_dataset r\n",
    "    ON o.order_id = r.order_id\n",
    "    AND EXTRACT(YEAR FROM o.order_purchase_timestamp) = cp.año_activo\n",
    "WHERE r.review_comment_message IS NOT NULL\n",
    "GROUP BY cp.customer_unique_id, cp.año_activo, r.review_score, r.review_comment_message\n",
    "ORDER BY cp.customer_unique_id, cp.año_activo\n",
    "\"\"\"\n",
    "database.sql(query).df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe8440",
   "metadata": {},
   "source": [
    "## Benchmark - En esta sección compararemos la performance de DuckDB contra su competidor más directo SQLite. Ambos utilizan a bases de datos embebidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327051c",
   "metadata": {},
   "source": [
    "### Realizaremos pruebas utilizando la base de datos almacenada en disco y en memoria.Las pruebas fueron realizadas en una Mac con M3 y 16GB de RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888321c2",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# En esta celda obtendremos todas las tablas que están en la bdd. \n",
    "# Para luego obtener las que tienen más filas, más columnas y las más pesadas en tamaño.\n",
    "# Utilizaremos estas tablas para realizar la comparación entre DuckDB y SQLite\n",
    "\n",
    "all_tables = [r[0] for r in database.sql(\"SHOW TABLES\").fetchall()]\n",
    "\n",
    "table_data = []\n",
    "# Se itera e las tablas, se obtienen los valores a medir y se guardan los resultados\n",
    "for table in all_tables:\n",
    "    rows_amount = database.sql(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "    cols_amount = len(database.sql(f\"PRAGMA table_info({table})\").fetchall())\n",
    "    csv = pathlib.Path(\"dataset\") / f\"{table}.csv\"\n",
    "    size = csv.stat().st_size if csv.exists() else 'En S3'\n",
    "    table_data.append((table, rows_amount, cols_amount, size))\n",
    "\n",
    "meta_df = pd.DataFrame(table_data, columns=[\"table\", \"rows\", \"cols\", \"size_bytes\"])\n",
    "# Las dos tablas que no están locales se ven en los resultados como 'En S3'\n",
    "# Sin embargo, lo verificamos y su peso no es mayor a la tabla de geolocalización\n",
    "display(meta_df)\n",
    "\n",
    "meta_df[\"size_num\"] = pd.to_numeric(meta_df[\"size_bytes\"], errors=\"coerce\")\n",
    "meta_df[\"size_num\"] = meta_df[\"size_num\"].fillna(0)\n",
    "\n",
    "most_rows = meta_df.loc[meta_df.rows.idxmax(), \"table\"]\n",
    "most_cols = meta_df.loc[meta_df.cols.idxmax(), \"table\"]\n",
    "most_size = meta_df.loc[meta_df.size_num.idxmax(), \"table\"]\n",
    "\n",
    "benchmark_tables = {\n",
    "    \"más_filas\":  most_rows,\n",
    "    \"más_cols\":   most_cols,\n",
    "    \"csv_pesado\": most_size,\n",
    "}\n",
    "\n",
    "print(f\"Tablas elegidas para el benchmark: {benchmark_tables}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9c44e",
   "metadata": {},
   "source": [
    "## Ejecución en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1437f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc, statistics as stats\n",
    "import  sqlite3\n",
    "\n",
    "# Elegimos consultas basicas pero pesadas para evaluar el rendimiento de ambos motores\n",
    "QUERIES = {\n",
    "    # Cuenta el número de filas de la tabla\n",
    "    \"row_count\" : \"SELECT COUNT(*) FROM t\",\n",
    "    # Agrupa por una columna y cuenta el número de filas en cada grupo\n",
    "    \"group_all\" : \"SELECT 1 g, COUNT(*) FROM t GROUP BY 1\",\n",
    "    # Selecciona filas al azar\n",
    "    \"filter\"    : \"SELECT * FROM t WHERE random() < 0.01\",\n",
    "    # Realiza un join de la tabla consigo misma\n",
    "    \"self_join\" : \"SELECT a.* FROM t a JOIN t b ON a.rowid = b.rowid LIMIT 1000\",\n",
    "}\n",
    "\n",
    "# Esta funcion ejecuta las consultas 5 veces y luego toma el tiempo para quedarse\n",
    "# con el promedio de esos tiempos\n",
    "def avg_time(factory, query_to_execute):                 \n",
    "    times = []\n",
    "    for i in range(5):\n",
    "        # El modulo gc permite invocar funciones de garbage collector para que las pruebas sean\n",
    "        # más \"justas\"\n",
    "        gc.collect()\n",
    "\n",
    "        # Esta fábrica se usara pa ejecutar la query con la BD nueva o la existente, \n",
    "        # de formma de poder variar las pruebas\n",
    "        connection = factory()    \n",
    "\n",
    "        # La funcion perf_counter permite medir el tiempo de ejecución                     \n",
    "        initial_time = time.perf_counter()                \n",
    "        query_to_execute(connection)\n",
    "\n",
    "        # Se agrega el tiempo de ejecución. Tiempo inicial menos actual                           \n",
    "        times.append(time.perf_counter() - initial_time)   \n",
    "\n",
    "        # Si la conexión es de las cold, se cierra\n",
    "        if getattr(connection, \"_close_after_run\", False):   \n",
    "            connection.close()\n",
    "\n",
    "    return stats.mean(times)\n",
    "\n",
    "def add_row(rows, table, query, state, duck_sec, sqlite_sec):\n",
    "    rows.append({\n",
    "        \"tabla\": table,\n",
    "        \"consulta\": query,\n",
    "        \"estado\": state,                     \n",
    "        \"tiempo duckdb (ms)\":  round(duck_sec   * 1_000, 3),\n",
    "        \"tiempo sqlite (ms)\":  round(sqlite_sec * 1_000, 3),\n",
    "    })\n",
    "\n",
    "bench_rows = []\n",
    "\n",
    "for table_characteristics, table in benchmark_tables.items():      \n",
    "    df = database.sql(f\"SELECT * FROM {table}\").fetchdf()\n",
    "\n",
    "    # Esta parte nos parecio un agregado interesante a testear.\n",
    "    # Ya que, no es lo mismo que la conexión con la base de datps esté ya activa a tener que levantarla\n",
    "    # desde 0 para ejecutar las consultas, entonces lo que hicimos fue crear una fábrica\n",
    "    # que permita utilizar dos base de datos, una \"fria\" que se inicia cada vez que se ejcuta\n",
    "    # la consulta y otra \"caliente\" que está siempre activa y se reutiliza.\n",
    "    # Esto tambien permite ver si hay diferencias en los distitnos tipos de bases,\n",
    "    # ya sea en cachés u otros datos que almacenen los motores para optimizar las consutlas.\n",
    "\n",
    "    # Definición de bases \"frias\"\n",
    "\n",
    "    cold_duck = lambda: duckdb.connect(\":memory:\")\n",
    "    cold_sql  = lambda: sqlite3.connect(\":memory:\")\n",
    "    cold_duck._close_after_run = True           \n",
    "    cold_sql._close_after_run  = True          \n",
    "\n",
    "    duck_ins_cold = avg_time(\n",
    "        cold_duck,\n",
    "        lambda c: (c.register(\"src\", df),\n",
    "                   c.execute(\"CREATE TABLE t AS SELECT * FROM src\"))\n",
    "    )\n",
    "    sqlite_ins_cold = avg_time(\n",
    "        cold_sql,\n",
    "        lambda c: df.to_sql(\"t\", c, if_exists=\"replace\", index=False)\n",
    "    )\n",
    "\n",
    "    # Definición de bases \"calientes\"\n",
    "\n",
    "    warm_duck = duckdb.connect(\":memory:\")       \n",
    "    warm_sql  = sqlite3.connect(\":memory:\")      \n",
    "    warm_duck.register(\"src\", df)\n",
    "    warm_duck.execute(\"CREATE TABLE t AS SELECT * FROM src\")\n",
    "    df.to_sql(\"t\", warm_sql, if_exists=\"replace\", index=False)\n",
    "\n",
    "    duck_ins_warm = avg_time(lambda: warm_duck,\n",
    "                             lambda c: c.execute(\"DELETE FROM t; INSERT INTO t SELECT * FROM src;\"))\n",
    "    sqlite_ins_warm = avg_time(lambda: warm_sql,\n",
    "                               lambda c: (c.execute(\"DELETE FROM t\"),\n",
    "                                          df.to_sql(\"t\", c, if_exists=\"append\", index=False)))\n",
    "\n",
    "    add_row(bench_rows, table_characteristics, \"insert\", \"cold\",\n",
    "            duck_ins_cold, sqlite_ins_cold)\n",
    "    add_row(bench_rows, table_characteristics, \"insert\", \"warm\",\n",
    "            duck_ins_warm, sqlite_ins_warm)\n",
    "\n",
    "    for query_name, query_to_exe in QUERIES.items():\n",
    "\n",
    "        duck_q_cold = avg_time(\n",
    "            cold_duck,\n",
    "            lambda c: (c.register(\"src\", df),\n",
    "                       c.execute(\"CREATE TABLE t AS SELECT * FROM src\"),\n",
    "                       c.execute(query_to_exe).fetchall())\n",
    "        )\n",
    "\n",
    "        cold_sql_factory = lambda: sqlite3.connect(\":memory:\")\n",
    "        cold_sql_factory._close_after_run = True\n",
    "\n",
    "        sqlite_q_cold = avg_time(\n",
    "            cold_sql_factory,\n",
    "            lambda c: (\n",
    "                df.to_sql(\"t\", c, if_exists=\"replace\", index=False),\n",
    "                c.execute(query_to_exe).fetchall()        \n",
    "            )\n",
    "        )\n",
    "\n",
    "        duck_q_warm = avg_time(lambda: warm_duck,\n",
    "                               lambda c: c.execute(query_to_exe).fetchall())\n",
    "        sqlite_q_warm = avg_time(lambda: warm_sql,\n",
    "                                 lambda c: c.execute(query_to_exe).fetchall())    \n",
    "\n",
    "        add_row(bench_rows, table_characteristics, query_name, \"cold\",\n",
    "                duck_q_cold,   sqlite_q_cold)\n",
    "        add_row(bench_rows, table_characteristics, query_name, \"warm\",\n",
    "                duck_q_warm,   sqlite_q_warm)\n",
    "\n",
    "    warm_duck.close(); warm_sql.close()\n",
    "\n",
    "res_df = pd.DataFrame(bench_rows)\n",
    "\n",
    "# Para que sea más visual, en caso que haya ganado DuckDB, se mostrará el valor positivo\n",
    "# y en caso que haya ganado SQLite, se mostrará el valor negativo de la diferencia.\n",
    "def diff(row):\n",
    "    duckdb_time, sqlite_time = row[\"tiempo duckdb (ms)\"], row[\"tiempo sqlite (ms)\"]\n",
    "    if duckdb_time < sqlite_time:                 \n",
    "        return round(sqlite_time / duckdb_time, 2)\n",
    "    else:                   \n",
    "        return - round(duckdb_time / sqlite_time, 2)\n",
    "\n",
    "res_df[\"diferencia (en veces más rápido)\"] = res_df.apply(diff, axis=1)\n",
    "display(res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909401b",
   "metadata": {},
   "source": [
    "### Visualización de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f422b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for query in res_df[\"consulta\"].unique():\n",
    "    sl = res_df[res_df[\"consulta\"] == query]\n",
    "    if sl.empty:\n",
    "        continue\n",
    "\n",
    "    sl = sl.sort_values([\"tabla\", \"estado\"])\n",
    "    x = np.arange(len(sl) // 2)          \n",
    "    width = 0.2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    cold_d = sl[sl[\"estado\"]==\"cold\"][\"tiempo duckdb (ms)\"].values\n",
    "    warm_d = sl[sl[\"estado\"]==\"warm\"][\"tiempo duckdb (ms)\"].values\n",
    "    cold_s = sl[sl[\"estado\"]==\"cold\"][\"tiempo sqlite (ms)\"].values\n",
    "    warm_s = sl[sl[\"estado\"]==\"warm\"][\"tiempo sqlite (ms)\"].values\n",
    "\n",
    "    ax.bar(x - 1.5*width, cold_d, width, label=\"DuckDB cold\")\n",
    "    ax.bar(x - 0.5*width, warm_d, width, label=\"DuckDB warm\")\n",
    "    ax.bar(x + 0.5*width, cold_s, width, label=\"SQLite cold\")\n",
    "    ax.bar(x + 1.5*width, warm_s, width, label=\"SQLite warm\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(sl[\"tabla\"].unique(), rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Milisegundos\")\n",
    "    ax.set_title(f\"{query} - en memoria - mayor es peor\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55627b65",
   "metadata": {},
   "source": [
    "## Ejecución en disco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff22fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_disk = []\n",
    "\n",
    "for key, tbl in benchmark_tables.items():\n",
    "    df = database.sql(f\"SELECT * FROM {tbl}\").fetchdf()\n",
    "    \n",
    "    # Creamos archivos temporales para DuckDB y SQLite \n",
    "    # para que sea más \"justo\"\n",
    "    \n",
    "    duck_file   = pathlib.Path(tempfile.mktemp(suffix=\".duckdb\"))\n",
    "    sqlite_file = pathlib.Path(tempfile.mktemp(suffix=\".sqlite\"))\n",
    "\n",
    "    # inserción única\n",
    "    duckdb.connect(duck_file).execute(\"CREATE TABLE t AS SELECT * FROM df\").close()\n",
    "    df.to_sql(\"t\", sqlite3.connect(sqlite_file), if_exists=\"replace\", index=False)\n",
    "\n",
    "    # en este caso, no tiene sentido diferenciar entre conexión activa o no,\n",
    "    # ya que estamos escribiendo en disco, por lo que no hay caché \n",
    "\n",
    "    duckdb_connection = duckdb.connect(duck_file)\n",
    "    sqlite_connection = sqlite3.connect(sqlite_file)\n",
    "\n",
    "    for qname, qsql in QUERIES.items():\n",
    "        initial_time = time.perf_counter(); duckdb_connection.execute(qsql).fetchall(); duck_ms = (time.perf_counter()-initial_time)*1e3\n",
    "        initial_time = time.perf_counter(); sqlite_connection.execute(qsql).fetchall(); sqlite_ms = (time.perf_counter()-initial_time)*1e3\n",
    "        rows_disk.append({\"tabla\": key, \"consulta\": qname,\n",
    "                          \"tiempo duckdb (ms)\": round(duck_ms,3),\n",
    "                          \"tiempo sqlite (ms)\": round(sqlite_ms,3)})\n",
    "\n",
    "    # Acá cerramos las conexiones como buena práctica, pero no es estrictamente necesario\n",
    "\n",
    "    duckdb_connection.close(); sqlite_connection.close()\n",
    "    duck_file.unlink(); sqlite_file.unlink()\n",
    "\n",
    "res_disk = pd.DataFrame(rows_disk)\n",
    "\n",
    "def diff(row):\n",
    "    duckdb_time = row[\"tiempo duckdb (ms)\"]\n",
    "    sqlite_time = row[\"tiempo sqlite (ms)\"]\n",
    "    if duckdb_time < sqlite_time:                 \n",
    "        return round(sqlite_time / duckdb_time, 2)\n",
    "    else:                   \n",
    "        return - round(duckdb_time / sqlite_time, 2)\n",
    "\n",
    "\n",
    "res_disk[\"diferencia\"] = res_disk.apply(diff, axis=1)\n",
    "\n",
    "display(res_disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3655cf72",
   "metadata": {},
   "source": [
    "### Visualización de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad911f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in res_disk[\"consulta\"].unique():\n",
    "    sl = res_disk[res_disk[\"consulta\"] == query].sort_values(\"tabla\")\n",
    "    x  = np.arange(len(sl))\n",
    "    width = 0.3\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.bar(x - width/2, sl[\"tiempo duckdb (ms)\"], width, label=\"DuckDB\")\n",
    "    ax.bar(x + width/2, sl[\"tiempo sqlite (ms)\"], width, label=\"SQLite\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(sl[\"tabla\"], rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Milisegundos\")\n",
    "    ax.set_title(f\"{query} - en disco - mayor es peor\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
